{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600137661156",
   "display_name": "Python 3.7.7 64-bit ('py3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "# Reverse source sequence is useful fo training\n",
    "def _tokenize_reverse(text):\n",
    "    return [char for char in text[::-1]]\n",
    "\n",
    "SRC = Field(\n",
    "    # tokenize=_tokenize_reverse, \n",
    "    tokenize=_tokenize,\n",
    "    init_token='<sos>', \n",
    "    eos_token='<eos>',\n",
    "    pad_token='<pad>',\n",
    "    lower=False\n",
    ")\n",
    "TRG = Field(\n",
    "    tokenize=_tokenize,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    pad_token='<pad>',\n",
    "    lower=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 54424 samples.\nTotal 43539 train samples.\nTotal 10885 valid samples.\n\n-2*(~x&y)-(~y)+2*(~x)-y+2\n2+y-)x~(*2+)y~(-)y&x~(*2-\n-2*(x|y)+1\n"
    }
   ],
   "source": [
    "exprs = torchtext.data.TabularDataset(\n",
    "    path='./dataset.csv',\n",
    "    format='csv',\n",
    "    fields=[\n",
    "        ('src', SRC),\n",
    "        ('trg', TRG)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data, valid_data = exprs.split(split_ratio=0.8)\n",
    "\n",
    "print(f'Total {len(exprs)} samples.')\n",
    "print(f'Total {len(train_data)} train samples.')\n",
    "print(f'Total {len(valid_data)} valid samples.')\n",
    "\n",
    "print()\n",
    "print(*exprs.examples[0].src, sep='')\n",
    "print(*exprs.examples[0].trg, sep='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 32 unique tokens in source vocabulary\nTotal 32 unique tokens in target vocabulary\n"
    }
   ],
   "source": [
    "# Build vocab only from the training set, which can prevent information leakage\n",
    "SRC.build_vocab(train_data)\n",
    "TRG.build_vocab(train_data)\n",
    "print(f'Total {len(SRC.vocab)} unique tokens in source vocabulary')\n",
    "print(f'Total {len(TRG.vocab)} unique tokens in target vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "device = torch.device('cuda')\n",
    "\n",
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size=batch_size,\n",
    "    sort=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)    \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        #src = [src_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src_len, batch_size, emb_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        #outputs = [src_len, batch_size, hid_dim * n_directions]\n",
    "        #hidden = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        #cell = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device   \n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]       \n",
    "         \n",
    "        for t in range(1, trg_len):            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)             \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1   \n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(32, 256)\n    (rnn): LSTM(256, 512, dropout=0.5)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(32, 256)\n    (rnn): LSTM(256, 512, dropout=0.5)\n    (fc_out): Linear(in_features=512, out_features=32, bias=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        # nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 3,186,720 trainable parameters\n"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):    \n",
    "    model.train()    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):        \n",
    "        src = batch.src\n",
    "        trg = batch.trg        \n",
    "        optimizer.zero_grad()        \n",
    "        output = model(src, trg)        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]        \n",
    "        output_dim = output.shape[-1]        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]        \n",
    "        loss = criterion(output, trg)        \n",
    "        loss.backward()        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        \n",
    "        optimizer.step()        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):    \n",
    "    model.eval()    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "            output_dim = output.shape[-1]            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            loss = criterion(output, trg)            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Epoch: 100, train loss: 0.004, val loss: 0.259: 100%|██████████| 100/100 [22:17<00:00, 13.37s/it]\n"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "pbar = tqdm(range(N_EPOCHS))\n",
    "for epoch in pbar:        \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    pbar.set_description(f'Epoch: {epoch+1}, train loss: {train_loss:.3f}, val loss: {valid_loss:.3f}')\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('valid_loss', valid_loss, epoch)\n",
    "    scheduler.step(valid_loss)   \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'lstm-{today}.pt')\n",
    "    \n",
    "    # print(f'Epochs: {epoch + 1}, Train Loss: {train_loss:.3f}, Val. Loss: {valid_loss:.3f}')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    model.eval()        \n",
    "\n",
    "    tokens = [token.lower() for token in sentence]\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]    \n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "            \n",
    "        pred_token = output.argmax(1).item()        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "src \t\t= 2*(x&(y|z))+(~x|~z)-2*(y^z)+(~y^z)-2*(y^~z)-(x&y)\ntrg \t\t= -(~x&(y^z))\npredicted trg \t= -(~x&(y^z))\n"
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "src = vars(valid_data.examples[idx])['src']\n",
    "trg = vars(valid_data.examples[idx])['trg']\n",
    "\n",
    "translation = translate(src, SRC, TRG, model, device)\n",
    "translation = ''.join(translation[:-1])\n",
    "\n",
    "# src = ''.join(src)[::-1]\n",
    "src = ''.join(src)\n",
    "trg = ''.join(trg)\n",
    "\n",
    "print(f'src \\t\\t= {src}')\n",
    "print(f'trg \\t\\t= {trg}')\n",
    "print(f'predicted trg \\t= {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy rate on train set: 0.506\nAccuracy rate on valid set: 0.473\n"
    }
   ],
   "source": [
    "def count_acc(dataset, SRC, TRG, model, device):\n",
    "    count = 0\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        src = vars(dataset.examples[idx])['src']\n",
    "        trg = vars(dataset.examples[idx])['trg']\n",
    "\n",
    "        translation = translate(src, SRC, TRG, model, device)\n",
    "        \n",
    "        if translation[:-1] == trg:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "train_acc_count = count_acc(train_data, SRC, TRG, model, device)\n",
    "print(f'Accuracy rate on train set: {train_acc_count/len(train_data):.3f}')\n",
    "\n",
    "valid_acc_count = count_acc(valid_data, SRC, TRG, model, device)\n",
    "print(f'Accuracy rate on valid set: {valid_acc_count/len(valid_data):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}