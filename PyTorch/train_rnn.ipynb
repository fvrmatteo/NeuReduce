{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "SRC = Field(\n",
    "    tokenize=_tokenize, \n",
    "    init_token='<sos>', \n",
    "    eos_token='<eos>',\n",
    "    pad_token='<pad>',\n",
    "    lower=False\n",
    ")\n",
    "TRG = Field(\n",
    "    tokenize=_tokenize,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    pad_token='<pad>',\n",
    "    lower=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 100000 train samples.\nTotal 10000 valid samples.\n\n-3*(~y)+2*(x|~y)-(~x)-(~x|y)-(~(x&y))-x-(x&~y)-(~x&y)\n-4*(x^y)-4*(~(x|y))\n-4*(x&y)-(x^y)+5*(~(x|y))-(~(x^y))-(~y)-(x|~y)-(~x)\n-4*x-3*y-1\n"
    }
   ],
   "source": [
    "train_data = torchtext.data.TabularDataset(\n",
    "    path='./train_data.csv',\n",
    "    format='csv',\n",
    "    fields=[\n",
    "        ('src', SRC),\n",
    "        ('trg', TRG)\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_data = torchtext.data.TabularDataset(\n",
    "    path='./test_data.csv',\n",
    "    format='csv',\n",
    "    fields=[\n",
    "        ('src', SRC),\n",
    "        ('trg', TRG)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f'Total {len(train_data)} train samples.')\n",
    "print(f'Total {len(valid_data)} valid samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 100000 samples.\nTotal 90000 train samples.\nTotal 10000 valid samples.\n\n-3*(~y)+2*(x|~y)-(~x)-(~x|y)-(~(x&y))-x-(x&~y)-(~x&y)\n-4*(x^y)-4*(~(x|y))\n"
    }
   ],
   "source": [
    "exprs = torchtext.data.TabularDataset(\n",
    "    path='./train_data.csv',\n",
    "    format='csv',\n",
    "    fields=[\n",
    "        ('src', SRC),\n",
    "        ('trg', TRG)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data, valid_data = exprs.split(split_ratio=0.9)\n",
    "\n",
    "print(f'Total {len(exprs)} samples.')\n",
    "print(f'Total {len(train_data)} train samples.')\n",
    "print(f'Total {len(valid_data)} valid samples.')\n",
    "\n",
    "print()\n",
    "print(*exprs.examples[0].src, sep='')\n",
    "print(*exprs.examples[0].trg, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 28 unique tokens in source vocabulary\nTotal 28 unique tokens in target vocabulary\n"
    }
   ],
   "source": [
    "# Build vocab only from the training set, which can prevent information leakage\n",
    "SRC.build_vocab(train_data)\n",
    "TRG.build_vocab(train_data)\n",
    "print(f'Total {len(SRC.vocab)} unique tokens in source vocabulary')\n",
    "print(f'Total {len(TRG.vocab)} unique tokens in target vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "device = torch.device('cuda')\n",
    "\n",
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size=batch_size,\n",
    "    sort=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)    \n",
    "        # self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        self.rnn = nn.RNN(emb_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        #src = [src_len, batch_size], 每一列是一个样本，每个样本长度固定为src_len(102)\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src_len, batch_size, emb_dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        #outputs = [src_len, batch_size, hid_dim * n_directions]\n",
    "        #hidden = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        #cell = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        #outputs are always from the top hidden layer\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)  \n",
    "        self.rnn = nn.RNN(emb_dim + hid_dim, hid_dim)  \n",
    "        # self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)        \n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, context):        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #context = [n layers * n directions, batch size, hid dim]        \n",
    "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        #context = [1, batch size, hid dim]        \n",
    "        input = input.unsqueeze(0)        \n",
    "        #input = [1, batch size]        \n",
    "        embedded = self.dropout(self.embedding(input))        \n",
    "        #embedded = [1, batch size, emb dim]                \n",
    "        emb_con = torch.cat((embedded, context), dim=2)       \n",
    "        #emb_con = [1, batch size, emb dim + hid dim]         \n",
    "        output, hidden = self.rnn(emb_con, hidden)        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]        \n",
    "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]        \n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim = 1)        \n",
    "        #output = [batch size, emb dim + hid dim * 2]        \n",
    "        prediction = self.fc_out(output)        \n",
    "        #prediction = [batch size, output dim]\n",
    "\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5): \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)        \n",
    "        #last hidden state of the encoder is the context\n",
    "        context = self.encoder(src)        \n",
    "        #context also used as the initial hidden state of the decoder\n",
    "        hidden = context        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]  \n",
    "              \n",
    "        for t in range(1, trg_len):            \n",
    "            #insert input token embedding, previous hidden state and the context state\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, context)            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 1,100,828 trainable parameters\n"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):    \n",
    "    model.train()    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):        \n",
    "        src = batch.src\n",
    "        trg = batch.trg        \n",
    "        optimizer.zero_grad()        \n",
    "        output = model(src, trg)        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]        \n",
    "        output_dim = output.shape[-1]        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]        \n",
    "        loss = criterion(output, trg)        \n",
    "        loss.backward()        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        \n",
    "        optimizer.step()        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):    \n",
    "    model.eval()    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "            output_dim = output.shape[-1]            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            loss = criterion(output, trg)            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Epoch: 10, train loss: 0.888, val loss: 3.780: 100%|██████████| 10/10 [05:30<00:00, 33.08s/it]\n"
    }
   ],
   "source": [
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "today = datetime.date.today()\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "pbar = tqdm(range(N_EPOCHS))\n",
    "for epoch in pbar: \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    pbar.set_description(f'Epoch: {epoch+1}, train loss: {train_loss:.3f}, val loss: {valid_loss:.3f}')\n",
    "    # writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    # writer.add_scalar('valid_loss', valid_loss, epoch)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'rnn-{today}.pt')\n",
    "    \n",
    "    # print(f'Epochs: {epoch + 1}, Train Loss: {train_loss:.3f}, Val. Loss: {valid_loss:.3f}')\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    model.eval()\n",
    "        \n",
    "    tokens = [token.lower() for token in sentence]\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        context = model.encoder(src_tensor)\n",
    "    \n",
    "    hidden = context\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden, context)\n",
    "            \n",
    "        pred_token = output.argmax(1).item()        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "src \t\t= 2*(x^y)-4*(x&~y^~z)+2*(~x|y)-4*(x&(y|~z))-(~x^~y&~z)+4*(x^~y|~z)-(~x|z)-(x^z)-(~x&z)-(x^~y&~z)\ntrg \t\t= 2*(~x&(y^z))-4*(x&(y^z))\npredicted trg \t= -(*(~x&(y^z))-(x&(y^z))\n"
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "src = vars(valid_data.examples[idx])['src']\n",
    "trg = vars(valid_data.examples[idx])['trg']\n",
    "\n",
    "translation = translate(src, SRC, TRG, model, device)\n",
    "translation = ''.join(translation[:-1])\n",
    "\n",
    "src = ''.join(src)\n",
    "trg = ''.join(trg)\n",
    "\n",
    "print(f'src \\t\\t= {src}')\n",
    "print(f'trg \\t\\t= {trg}')\n",
    "print(f'predicted trg \\t= {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 10000/10000 [01:01<00:00, 162.24it/s]Accuracy rate on valid set: 0.001, count:     6/10000\n\n"
    }
   ],
   "source": [
    "def count_acc(dataset, SRC, TRG, model, device):\n",
    "    count = 0\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        src = vars(dataset.examples[idx])['src']\n",
    "        trg = vars(dataset.examples[idx])['trg']\n",
    "\n",
    "        translation = translate(src, SRC, TRG, model, device)\n",
    "        \n",
    "        if translation[:-1] == trg:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# train_acc_count = count_acc(train_data, SRC, TRG, model, device)\n",
    "# print(f'Accuracy rate on train set: {train_acc_count/len(train_data):.3f}, count: {train_acc_count:>5d}/{len(train_data):>5d}')\n",
    "\n",
    "valid_acc_count = count_acc(valid_data, SRC, TRG, model, device)\n",
    "print(f'Accuracy rate on valid set: {valid_acc_count/len(valid_data):.3f}, count: {valid_acc_count:>5d}/{len(valid_data):>5d}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('py3': conda)",
   "language": "python",
   "name": "python_defaultSpec_1600754373116"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}